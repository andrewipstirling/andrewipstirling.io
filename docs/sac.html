---
layout: default
title: About Me
---

<h2>Reinforcement Learning for Inverted Pendulum Control</h2>


<section>
  <p>
      Control theory is centered around the behavior of dynamic systems, and how as engineers we can force it to behave in a 
      certain way. In the context of Mechanical Engineering, these systems could vary from robotics, to automotive systems, to aerospace 
      and HVAC applications. A large portion of Control design is spent modeling the system or "plant". This requires a good grasp of 
      the system's mechanics, understanding all variables that may influence its behavior. For complex systems, these dynamics are highly 
      non-linear, while exhibiting large degrees of freedom. In such applications, traditional control techniques may struggle to produce 
      reliable and accurate behavior. Moreover, we may not even be able to model the system, making any form of controller design impossible.
  </p>

  <p>
      In these cases, we can make use of Reinforcement Learning (RL) for controlling such complex systems. RL operates in a 
      model-free fashion, meaning we are not required to have an explicit model of a system's behavior given various inputs. Using RL, 
      we can effectively sidestep all the disadvantages of model-based controller design. RL generally consists of an agent who, through 
      interacting with an environment, can learn to map various situations to actions. Specifically, the environment provides responses to 
      each agent's action, using these responses and rewards dependent on various states of the agent, we can condition the agent's decisions 
      towards optimal states. In class, the problems we solved required the action and/or the state to be discretizable. However, for 
      continuous systems, this may not be possible or if it is, it raises the question as to what resolution we should discretize the space 
      or actions. The Soft Actor-Critic architecture is a form of RL that looks to solve this problem.
  </p>

  <p>
      In this project, I explored RL's applications and performance in control settings. Specifically, I investigated
      the Soft Actor-Critic (SAC) architecture's ability to control a continuous system, consisting of an inverted pendulum 
      on a cart. With random control inputs, we can see the Pendulum falls nearly immediately, and we must seek to how keep it 
      upright using RL.
  </p>

  <img src="https://github.com/andrewipstirling/andrewipstirling.io/assets/77990898/448ff138-176e-4c49-b6b4-47f011984b70" alt="Inverted Pendulum Control Image">

  <h3>Project</h3>

  <p>
      This project implements the SAC framework from <a href="https://github.com/openai/spinningup">https://github.com/openai/spinningup</a>, 
      for the inverted pendulum in the gymnasium environment found at 
      <a href="https://github.com/Farama-Foundation/Gymnasium">https://github.com/Farama-Foundation/Gymnasium</a>. The final presentation 
      can be found in the <i>soft_actor_critic.ipynb</i> notebook of my repository at 
      <a href="https://github.com/andrewipstirling/sac_inverted_pendulum">https://github.com/andrewipstirling/sac_inverted_pendulum</a>, with 
      all code in the relevant files. Upon completion, I was able to obtain successful control of the pendulum, as can be seen below.
  </p>

  <img src="https://github.com/andrewipstirling/andrewipstirling.io/assets/77990898/98ac38bb-3d65-4b04-850b-7762ab135039" alt="Reinforcement Learning Control Image">

</section>

</html>